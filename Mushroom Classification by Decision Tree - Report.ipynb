{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search: The Grid Search one that we have discussed above usually increases the complexity in terms of the computation flow, So sometimes GS is considered inefficient since it attempts all the combinations of given hyperparameters.  But the Randomized Search is used to train the models based on random hyperparameters and combinations. obviously, the number of training models are small column than grid search.\n",
    "\n",
    "In simple terms, In Random Search, in a given grid, the list of hyperparameters are trained and test our model on a random combination of given hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "For y variable encoding is done as Poisonous = p -> 1 Edible = e -> 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbaspour - 610398147 - HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to see how decision tree classifying works for mushroom dataset.\n",
    "At first we import necessary methods from useful libraries.\n",
    "Time for computing how long does modeling takes.\n",
    "Some methods from 'pandas' to acess .csv format dataframe.\n",
    "some sklearn methods for manipulating data located in dataset and evaluate what would be done.\n",
    "Matplotlib and seaborn methods are used for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pandas import read_csv, get_dummies # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text #, plot_tree, export_graphviz\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from matplotlib.pyplot import subplots, title, fill_between, xlabel, ylabel, figure, show\n",
    "from seaborn import histplot, heatmap, countplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hand-made funciton is called four times for four states 'GridSearchCV' and 'RandomizedSearchCV' for two sort of data preprocessings: 'One Hot Encoding' (which causes cursed dimensionality from 22 to 117) and 'Label Encoding Only'.\n",
    "The function has the raw Model for hyper parameterizaiton(Fine tuning) and first, optimize the parameters by being applied with validation set and then, gets trained with train set.\n",
    "At last, evaluation parameters and also confusion matrices for each splitted dataset will be contructed.\n",
    "Also texual representation for each decision tree is available.\n",
    "Despite figuring out best possible criteria and maximum depths, it finds out optimal values for other parameters as bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def Modeling(Model):\n",
    "    startTime = time()\n",
    "    # Obtaining best paramteres by using splitted validation data\n",
    "    Model.fit(X_val, y_val)\n",
    "    Duration = time() - startTime # Calculating how long fitting train data takes\n",
    "\n",
    "    print(f\"\\n{Model.best_score_}\")\n",
    "    print(f\"\\n{Model.best_params_}\")\n",
    "    Model = Model.best_estimator_ # = DecisionTreeClassifier(Optimal Parameters)\n",
    "    print(f\"\\n{Model}\")\n",
    "\n",
    "    Model.fit(X_train, y_train)\n",
    "    y_pred = Model.predict(X_test)\n",
    "    print(f\"\\nModel Score: {Model.score(X_test, y_pred)}\")\n",
    "    print(f\"\\nAccuracy Score: {accuracy_score(y_test, y_pred)}\" )\n",
    "    print(f\"\\nPrecision Score: {precision_score(y_test, y_pred)}\")\n",
    "    print(f\"\\nRecall Score: {recall_score(y_test, y_pred)}\")\n",
    "    print(f\"\\nF1-Score: {f1_score(y_test,y_pred)}\")\n",
    "    print(f\"\\nComputation Time: {Duration}\")\n",
    "\n",
    "    # Confusion matrix for train set\n",
    "    _pred = Model.predict(X_train)\n",
    "    ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_train, _pred), display_labels = [\"True\", \"False\"]).plot()\n",
    "    show()\n",
    "\n",
    "    # Confusion matrix for validation set\n",
    "    _pred = Model.predict(X_val)\n",
    "    ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_val, _pred), display_labels = [\"True\", \"False\"]).plot()\n",
    "    show()\n",
    "\n",
    "    # Confusion matrix for test set\n",
    "    ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred), display_labels = [\"True\", \"False\"]).plot()\n",
    "    show()\n",
    "\n",
    "    print(\"Decision Tree Classifier report: \\n\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Texual representation of modelled decision tree\n",
    "    print(export_text(Model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening main dataset and plotting basic informations about it containing class distribution graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = read_csv(\"mushrooms.csv\").copy()\n",
    "\n",
    "# Decribing raw data records\n",
    "nSamples, nFeatures = df.shape\n",
    "print(f\"Data record has {nSamples} samples and {nFeatures} features.\\n\")\n",
    "df.describe()\n",
    "df.info()\n",
    "df.isnull().values.any() # Check if any missing value exists.\n",
    "\n",
    "# Plotting raw data records\n",
    "Figure, ax = subplots()\n",
    "ax.pie(df[\"class\"].value_counts(), labels = [\"Edible\", \"Poisonous\"], autopct = '%1.1f%%', shadow = True, startangle = 90)\n",
    "ax.axis('equal')\n",
    "show()\n",
    "histplot(df['class'])\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we model two fine tuning methods for two preprocessed splitted data.\n",
    "Dataset is splitted by 70, 15, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing data by means of Label Enconding & One Hot Encoding\n",
    "labelEncoder, Features = LabelEncoder(), df.columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(get_dummies(df.drop(['class'], axis = 1)), labelEncoder.fit_transform(df['class']), test_size = 0.3, random_state = 42) # 70:30\n",
    "\n",
    "# second, split into X_val, X_test, y_val, y_test\n",
    "# 'test_size=0.5' split into 50% and 50%. The original data set is 30%; so, it will split into 15% equally.\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42) # 70:15:15\n",
    "\n",
    "# splitted dataset dimensions\n",
    "print(\"\\nDimension of splitted datasets are listed below:\")\n",
    "print(f\"\\nTrain sets: {X_train.shape}\", y_train.shape)\n",
    "print(f\"\\nValidation sets: {X_val.shape}\", y_val.shape)\n",
    "print(f\"\\nTest sets: {X_test.shape}\", y_test.shape)\n",
    "\n",
    "# Train model by Grid Search hyper parameterization approach\n",
    "print(\"\\nGridSearchCV:\")\n",
    "Modeling(GridSearchCV(estimator = DecisionTreeClassifier(random_state = 42), param_grid = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}, verbose = 1, cv = 3, scoring = \"accuracy\", error_score= \"raise\"))\n",
    "\n",
    "# Train model by Randomized Search hyper parameterization approach\n",
    "print(\"\\nRandomizedSearchCV:\")\n",
    "Modeling(RandomizedSearchCV(DecisionTreeClassifier(), {\"criterion\": [\"gini\", \"entropy\"], \"max_features\": [\"sqrt\", \"log2\"], \"min_samples_leaf\": range(1, 100, 1), \"max_depth\": range(1, 50, 1)}, cv = 10, scoring = \"accuracy\", n_iter = 20, random_state = 5))\n",
    "\n",
    "# Label Enconding Only\n",
    "for Feature in Features:\n",
    "    df[Feature] = labelEncoder.fit_transform(df[Feature])\n",
    "\n",
    "# Convert data to have zero mean and unit variance\n",
    "X_train, X_test, y_train, y_test = train_test_split(StandardScaler().fit_transform(df.iloc[:,1:18]), df.iloc[:, 0], test_size=0.3, random_state = 42) # 70:30\n",
    "\n",
    "# 'test_size=0.5' split into 50% and 50%. The original data set is 30%; so, it will split into 15% equally.\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42) # 70:15:15\n",
    "\n",
    "# Train model by Grid Search hyper parameterization approach\n",
    "print(\"\\nGridSearchCV:\")\n",
    "Modeling(GridSearchCV(estimator = DecisionTreeClassifier(random_state = 42), param_grid = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}, verbose = 1, cv = 3, scoring = \"accuracy\", error_score= \"raise\"))\n",
    "\n",
    "# Train model by Randomized Search hyper parameterization approach\n",
    "print(\"\\nRandomizedSearchCV:\")\n",
    "Modeling(RandomizedSearchCV(DecisionTreeClassifier(), {\"criterion\": [\"gini\", \"entropy\"], \"max_features\": [\"sqrt\", \"log2\"], \"min_samples_leaf\": range(1, 100, 1), \"max_depth\": range(1, 50, 1)}, cv = 10, scoring = \"accuracy\", n_iter = 20, random_state = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis\n",
    "In the prior data set, edible mushrooms were recorded a bit more than poisonous mushrooms were.\n",
    "Any detail about whole results obtained from generating whole modeling types are illustrated in 'Evalutaions' folder.\n",
    "According to that:\n",
    "Most accurate confusion matrix for:\n",
    "1. Train set was in RandomizedSearchCV for full-preprocessed data and GridSearchCV for label-encoded only data modeling types\n",
    "2. Validation set was in GridSearchCV for label-encoded only data modeling type\n",
    "3. Test set was in RandomizedSearchCV for full-preprocessed data modeling type\n",
    "\n",
    "Least accurate confusion matrix for:\n",
    "1. Train set was in GridSearchCV for full-preprocessed data modeling type\n",
    "2. Validation set was in RandomizedSearchCV for full-preprocessed data modeling type\n",
    "3. Test set was in GridSearchCV for full-preprocessed data modeling type\n",
    "\n",
    "According to their scores, the best modeling type is GridSearchCV for full-preprocessed data modeling and the worst one is RandomizedSearchCV for label-encoded only data.\n",
    "\n",
    "When working with a mushroom dataset, especially for classification tasks such as determining whether a mushroom is edible or poisonous, selecting the most informative features for comparison is crucial. Here are some of the best features typically used in the analysis of mushroom datasets:\n",
    "\n",
    "1. **Cap Shape:** The shape of the mushroom cap (e.g., bell, conical, flat, convex) can be a significant feature for classification.\n",
    "\n",
    "2. **Cap Surface:** The surface texture of the mushroom cap (e.g., fibrous, grooves, scaly, smooth) can provide important information.\n",
    "\n",
    "3. **Cap Color:** The color of the mushroom cap is a key feature for identification and classification.\n",
    "\n",
    "4. **Bruises:** Whether the mushroom bruises when damaged can be a distinguishing characteristic.\n",
    "\n",
    "5. **Odor:** Mushroom odor (e.g., almond, anise, creosote, fishy, foul) is a critical feature for differentiating between species.\n",
    "\n",
    "6. **Gill Attachment:** The attachment of gills to the stem (e.g., free, attached, notched) is a relevant feature.\n",
    "\n",
    "7. **Gill Spacing:** The spacing between gills on the underside of the cap (e.g., close, crowded, distant) can be informative.\n",
    "\n",
    "8. **Gill Size:** The size of gills in relation to the cap diameter is a valuable feature.\n",
    "\n",
    "9. **Stalk Shape:** The shape of the mushroom stalk (e.g., enlarging, tapering) is an important characteristic.\n",
    "\n",
    "10. **Stalk Surface Above Ring:** The texture of the stalk surface above the ring (e.g., fibrous, scaly, smooth) provides useful insights.\n",
    "\n",
    "11. **Stalk Color Above Ring:** The color of the stalk above the ring is a distinguishing feature.\n",
    "\n",
    "12. **Stalk Color Below Ring:** Similarly, the color of the stalk below the ring can be indicative of species.\n",
    "\n",
    "13. **Veil Color:** The color of the veil covering the gills (e.g., brown, orange, white) is a relevant feature.\n",
    "\n",
    "14. **Ring Number:** The number of rings on the stalk can be a distinguishing feature.\n",
    "\n",
    "15. **Spore Print Color:** The color of the spore print left by the mushroom can aid in identification.\n",
    "\n",
    "When comparing features in a mushroom dataset, it is essential to select those that are most discriminatory and relevant for distinguishing between different classes (edible vs. poisonous). Feature selection techniques such as correlation analysis, mutual information, and feature importance from machine learning models can help identify the most informative features in the dataset. Conducting exploratory data analysis and considering domain knowledge can also provide insights into the best features for comparison in a mushroom dataset.\n",
    "\n",
    "Challenges:\n",
    "Main challenge was to preprocess raw data and made features numerical.\n",
    "Minor challenge was about plotting.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
